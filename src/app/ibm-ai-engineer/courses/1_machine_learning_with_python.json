{
  "moduleNumber": 1,
  "title": "Machine Learning with Python",
  "about": "Covers regression, classification, clustering, pipelines, model evaluation, and dimensionality reduction using Python and scikit-learn.",
  "moduleDetails": [
    {
      "moduleNumber": 2,
      "title": "Regression Models",
      "description": "Covers simple regression, multiple linear regression, and logistic regression",
      "summary": "Regression models relationships between a continuous target variable and explanatory features, covering simple and multiple regression types. Simple regression uses a single independent variable to estimate a dependent variable, while multiple regression involves more than one independent variable. Regression is widely applicable, from forecasting sales and estimating maintenance costs to predicting rainfall and disease spread. In simple linear regression, a best-fit line minimizes errors, measured by Mean Squared Error (MSE); this approach is known as Ordinary Least Squares (OLS). OLS regression is easy to interpret but sensitive to outliers, which can impact accuracy. Multiple linear regression extends simple linear regression by using multiple variables to predict outcomes and analyze variable relationships. Adding too many variables can lead to overfitting, so careful variable selection is necessary to build a balanced model. Nonlinear regression models complex relationships using polynomial, exponential, or logarithmic functions when data does not fit a straight line. Polynomial regression can fit data but may overfit by capturing random noise rather than underlying patterns. Logistic regression is a probability predictor and binary classifier, suitable for binary targets and assessing feature impact. Logistic regression minimizes errors using log-loss and optimizes with gradient descent or stochastic gradient descent for efficiency. Gradient descent is an iterative process to minimize the cost function, which is crucial for training logistic regression models.",
      "labs": [
        {
          "name": "Simple Regression",
          "file": "module 2/Simple-Linear-Regression.ipynb",
          "description": "Implementing simple linear regression to model relationships between variables"
        },
        {
          "name": "Multiple Linear Regression",
          "file": "module 2/Mulitple-Linear-Regression.ipynb",
          "description": "Extending simple regression to multiple independent variables"
        },
        {
          "name": "Logistic Regression",
          "file": "module 2 Logistic Regression/Logistic_Regression.ipynb",
          "description": "Using logistic regression for binary classification problems"
        }
      ]
    },
    {
      "moduleNumber": 3,
      "title": "Classification Methods",
      "description": "Covers decision trees, random forests, KNN, and SVM classification techniques",
      "summary": "Classification is a supervised machine learning method used to predict labels on new data with applications in churn prediction, customer segmentation, loan default prediction, and multiclass drug prescriptions. Binary classifiers can be extended to multiclass classification using one-versus-all or one-versus-one strategies. A decision tree classifies data by testing features at each node, branching based on test results, and assigning classes at leaf nodes. Decision tree training involves selecting features that best split the data and pruning the tree to avoid overfitting. Information gain and Gini impurity are used to measure the quality of splits in decision trees. Regression trees are similar to decision trees but predict continuous values by recursively splitting data to maximize information gain. Mean Squared Error (MSE) is used to measure split quality in regression trees. K-Nearest Neighbors (k-NN) is a supervised algorithm used for classification and regression by assigning labels based on the closest labeled data points. To optimize k-NN, test various k values and measure accuracy, considering class distribution and feature relevance. Support Vector Machines (SVM) build classifiers by finding a hyperplane that maximizes the margin between two classes, effective in high-dimensional spaces but sensitive to noise and large datasets. The bias-variance tradeoff affects model accuracy, and methods such as bagging, boosting, and random forests help manage bias and variance to improve model performance. Random forests use bagging to train multiple decision trees on bootstrapped data, improving accuracy by reducing variance.",
      "labs": [
        {
          "name": "Multi-class Classification",
          "file": "module 3/Multi-class_Classification.ipynb",
          "description": "Implementing classifiers for problems with multiple classes"
        },
        {
          "name": "Decision Trees",
          "file": "module 3/Decision_trees.ipynb",
          "description": "Building and optimizing decision tree classifiers"
        },
        {
          "name": "Regression Trees",
          "file": "module 3/Regression_Trees_Taxi_Tip.ipynb",
          "description": "Using decision trees for regression tasks with taxi tip prediction"
        },
        {
          "name": "Decision Trees and SVM",
          "file": "module 3/decision_tree_svm_ccFraud.ipynb",
          "description": "Comparing decision trees and SVM for credit card fraud detection"
        },
        {
          "name": "KNN Classification",
          "file": "module 3/KNN_Classification.ipynb",
          "description": "Implementing K-Nearest Neighbors algorithm for classification"
        },
        {
          "name": "Random Forests & XGBoost",
          "file": "module 3/Random_%20Forests%20_XGBoost.ipynb",
          "description": "Implementing ensemble learning with Random Forests and XGBoost"
        }
      ]
    },
    {
      "moduleNumber": 4,
      "title": "Clustering and Dimensionality Reduction",
      "description": "Covers K-means clustering, DBSCAN, hierarchical clustering, and dimensionality reduction techniques",
      "summary": "Clustering is a machine learning technique used to group data based on similarity, with applications in customer segmentation and anomaly detection. K-means clustering partitions data into clusters based on the distance between data points and centroids but struggles with imbalanced or non-convex clusters. Heuristic methods such as silhouette analysis, the elbow method, and the Davies-Bouldin Index help assess k-means performance. DBSCAN is a density-based algorithm that creates clusters based on density and works well with natural, irregular patterns. HDBSCAN is a variant of DBSCAN that does not require parameters and uses cluster stability to find clusters. Hierarchical clustering can be divisive (top-down) or agglomerative (bottom-up) and produces a dendrogram to visualize the cluster hierarchy. Dimension reduction simplifies data structure, improves clustering outcomes, and is useful in tasks such as face recognition (using eigenfaces). Clustering and dimension reduction work together to improve model performance by reducing noise and simplifying feature selection. PCA, a linear dimensionality reduction method, minimizes information loss while reducing dimensionality and noise in data. t-SNE and UMAP are other dimensionality reduction techniques that map high-dimensional data into lower-dimensional spaces for visualization and analysis.",
      "labs": [
        {
          "name": "K-Means Clustering",
          "file": "module 4/K-Means-Customer-Seg.ipynb",
          "description": "Implementing K-means clustering for customer segmentation"
        },
        {
          "name": "Comparing DBSCAN and HDBSCAN",
          "file": "module 4/Comparing_DBScan_HDBScan.ipynb",
          "description": "Comparing density-based clustering algorithms"
        },
        {
          "name": "PCA",
          "file": "module 4/PCA.ipynb",
          "description": "Implementing Principal Component Analysis for dimensionality reduction"
        },
        {
          "name": "t-SNE and UMAP",
          "file": "module 4/tSNE_UMAP.ipynb",
          "description": "Exploring advanced dimensionality reduction techniques"
        }
      ]
    },
    {
      "moduleNumber": 5,
      "title": "Model Evaluation and Pipelines",
      "description": "Covers model evaluation metrics, regularization techniques, and ML pipelines",
      "summary": "Supervised learning evaluation assesses a model's ability to predict outcomes for unseen data, often using a train/test split to estimate performance. Key metrics for classification evaluation include accuracy, confusion matrix, precision, recall, and the F1 score, which balances precision and recall. Regression model evaluation metrics include MAE, MSE, RMSE, R-squared, and explained variance to measure prediction accuracy. Unsupervised learning models are evaluated for pattern quality and consistency using metrics like Silhouette Score, Davies-Bouldin Index, and Adjusted Rand Index. Dimensionality reduction evaluation involves Explained Variance Ratio, Reconstruction Error, and Neighborhood Preservation to assess data structure retention. Model validation, including dividing data into training, validation, and test sets, helps prevent overfitting by tuning hyperparameters carefully. Cross-validation methods, especially K-fold and stratified cross-validation, support robust model validation without overfitting to test data. Regularization techniques, such as ridge (L2) and lasso (L1) regression, help prevent overfitting by adding penalty terms to linear regression models. Data leakage occurs when training data includes information unavailable in real-world data, which is preventable by separating data properly and mindful feature selection. Common modeling pitfalls include misinterpreting feature importance, ignoring class imbalance, and relying excessively on automated processes without causal analysis. Feature importance assessments should consider redundancy, scale sensitivity, and avoid misinterpretation, as well as inappropriate assumptions about causation.",
      "labs": [
        {
          "name": "Evaluating Classification Models",
          "file": "module 5/Evaluating_Classification_Models_v1.ipynb",
          "description": "Evaluating and comparing different classification models"
        },
        {
          "name": "Evaluating Random Forest",
          "file": "module 5/Evaluating_random_forest_v1.ipynb",
          "description": "In-depth evaluation of Random Forest models"
        },
        {
          "name": "Evaluating K-means Clustering",
          "file": "module 5/Evaluating_k_means_clustering_v1.ipynb",
          "description": "Techniques for evaluating K-means clustering algorithms"
        },
        {
          "name": "Regularization in Linear Regression",
          "file": "module 5/Regularization_in_LinearRegression_v1.ipynb",
          "description": "Implementing ridge and lasso regularization in regression models"
        },
        {
          "name": "ML Pipelines and GridSearchCV",
          "file": "module 5/ML_Pipelines_and_GridSearchCV.ipynb",
          "description": "Building machine learning pipelines and hyperparameter tuning"
        }
      ]
    },
    {
      "moduleNumber": 6,
      "title": "Final Projects",
      "description": "Practice project and final capstone project",
      "summary": "The final module focuses on applying all learned concepts to complete machine learning projects from start to finish. Students work on practice and final projects that involve data preprocessing, feature engineering, model selection and tuning, evaluation, and interpretation of results.",
      "labs": [
        {
          "name": "Practice Project",
          "file": "module 6/Practice_Project_v1.ipynb",
          "description": "Practice applying machine learning concepts to a complete project"
        },
        {
          "name": "Final Project",
          "file": "module 6/FinalProject_AUSWeather.ipynb",
          "description": "Capstone project focused on Australian weather prediction"
        }
      ]
    }
  ],
  "topics": [
    {
      "name": "Regression",
      "details": "Simple and multiple linear regression, model evaluation, and prediction."
    },
    {
      "name": "Classification",
      "details": "Logistic regression, decision trees, random forests, and XGBoost for classification tasks."
    },
    {
      "name": "Clustering",
      "details": "K-Means clustering for unsupervised learning and customer segmentation."
    },
    {
      "name": "Dimensionality Reduction",
      "details": "Principal Component Analysis (PCA) for reducing feature space."
    },
    {
      "name": "Pipelines & Model Selection",
      "details": "Building ML pipelines and using GridSearchCV for hyperparameter tuning."
    }
  ],
  "labs": [
    {
      "name": "Simple Regression",
      "file": "module 2/Simple-Linear-Regression.ipynb"
    },
    {
      "name": "Multiple Linear Regression",
      "file": "module 2/Mulitple-Linear-Regression.ipynb"
    },
    {
      "name": "Logistic Regression",
      "file": "module 2 Logistic Regression/Logistic_Regression.ipynb"
    },
    { "name": "Decision Trees", "file": "module 3/Decision_trees.ipynb" },
    {
      "name": "Random Forests & XGBoost",
      "file": "module 3/Random_%20Forests%20_XGBoost.ipynb"
    },
    {
      "name": "K-Means Clustering",
      "file": "module 4/K-Means-Customer-Seg.ipynb"
    },
    { "name": "PCA", "file": "module 4/PCA.ipynb" },
    {
      "name": "ML Pipelines & GridSearchCV",
      "file": "module 5/ML_Pipelines_and_GridSearchCV.ipynb"
    },
    {
      "name": "Practice Project",
      "file": "module 6/Practice_Project_v1.ipynb"
    },
    {
      "name": "Final Project",
      "file": "module 6/FinalProject_AUSWeather.ipynb"
    }
  ],
  "summary": "This module provides a comprehensive introduction to machine learning using Python, covering both supervised and unsupervised learning techniques, model evaluation, and practical implementation with real datasets."
}
